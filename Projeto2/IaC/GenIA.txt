========================== Documentação do Sistema de Pipeline ML =======================================

A estrutura do seu projeto Projeto2 está muito bem organizada e segue boas práticas de Engenharia de Dados e DevOps.

Funcionamento:

Como o EMR roda em um cluster na nuvem da AWS, ele não consegue ler esse arquivo diretamente da sua máquina local. 
O fluxo ideal para o seu projeto será:

1. Local: Você tem o dataset.csv na pasta dados/.

2. Upload: O Terraform (ou um script de pré-execução) deve subir esse arquivo para o Bucket S3 que você criou no módulo s3.

3. Leitura: O script p2_processamento.py (ou p2_ml.py) vai ler o arquivo do S3 (s3://seu-bucket/dataset.csv), e não do caminho local


O S3 é o coração da sua "Data Lake" nessa arquitetura.

No contexto de EMR e Spark, essa separação entre Armazenamento (S3) e Computação (EMR) é fundamental.

Persistência: Se você desligar o cluster EMR (o que é ideal para economizar custos após o treino), seus dados e o modelo treinado persistem no S3.

HDFS vs S3: O Spark no EMR usa o conector s3a:// (ou s3:// otimizado pelo EMRFS) para ler/escrever diretamente no bucket, tratando-o quase como se fosse um sistema 
de arquivos local.


Basicamente, o seu fluxo de dados (Pipeline) parece funcionar assim:

[Ingestão] (p2_processamento.py): Pega o dataset.csv local, processa (limpeza/transformação inicial) e faz o upload para o S3. Isso é crucial: o EMR não precisa 
"ver" seu disco local, ele só precisa ver o S3.

[Treinamento] (p2_ml.py): O cluster EMR (via Spark) lê os dados já processados do S3, treina a Regressão Logística de forma distribuída e provavelmente salva o 
modelo de volta no S3.

[Orquestração] (projeto2.py): É o "maestro" que garante que o passo 1 termine com sucesso antes do passo 2 começar.

[Observabilidade] (p2_log.py): Mantém o registro do que aconteceu, fundamental para debugar falhas em clusters distribuídos onde ler logs brutos do YARN pode ser difícil.

Essa estrutura desacoplada é muito profissional. Se você precisar trocar o modelo de ML no futuro (ex: para uma Random Forest), você só mexe no p2_ml.py sem quebrar
 o upload de dados.



O bootstrap.sh é a peça que garante a consistência do seu cluster.

Sem ele, o seu script PySpark (p2_ml.py) falharia nos nós workers (Core/Task) assim que tentasse importar uma biblioteca que não é nativa do EMR 
(como scikit-learn ou uma versão específica do pandas), pois o driver (Master) teria a lib, mas os executores não.


O fluxo que você montou está muito sólido:

Provisionamento: O Terraform sobe as máquinas EC2.

Bootstrap (scripts/bootstrap.sh): Antes do Spark iniciar, esse script roda em todos os nós para instalar as dependências Python.

Execução: O Spark inicia e encontra o ambiente pronto.

IaC/modules/emr/main.tf (Para ver a definição do cluster e do bootstrap)

scripts/bootstrap.sh (Para ver o que está sendo instalado)

pipeline/p2_ml.py (Para ver a lógica de ML distribuído)


O uso de Remote State no S3 (config.tf) é uma prática profissional indispensável para garantir que o estado da infraestrutura esteja seguro e consistente, 
especialmente se você trabalhar em equipe ou via CI/CD no futuro.

Agora que mapeamos toda a estrutura e entendi como as peças se conectam (S3 <-> EMR <-> Pipeline), vamos mergulhar no código para garantir que a lógica reflete 
essa arquitetura.


Script main.tf (Orquestração do projeto)

O código do seu IaC/main.tf está limpo e bem estruturado. A passagem de variáveis via terraform.tfvars é a prática correta para manter dados sensíveis ou 
específicos do ambiente fora do código versionado.

No entanto, identifiquei um ponto crítico de arquitetura aqui que pode causar falha no provisionamento do EMR.

O Risco de "Race Condition" (Condição de Corrida)
O Terraform constrói um grafo de dependências para saber a ordem de criação.

Você passa var.name_bucket para o module "s3".

Você passa a mesma variável var.name_bucket para o module "emr".

O problema: Como ambos usam uma variável de entrada (e não uma saída um do outro), o Terraform pode tentar criar o cluster EMR ao mesmo tempo (em paralelo) que cria o Bucket S3 e faz o upload dos arquivos.

Consequência: O EMR pode tentar ler o script de bootstrap (bootstrap.sh) no S3 antes de o upload ter terminado, resultando em erro de inicialização do cluster.

A Solução Recomendada
Para garantir que o EMR só comece a ser criado depois que o S3 estiver pronto e com os arquivos (files_bash, files_data) carregados, você tem duas opções. 
A mais explícita é adicionar um depends_on:


# Exemplo:

module "emr" {
  source                  = "./modules/emr"
  # ... suas variáveis ...
  
  # Adicione esta linha:
  depends_on              = [module.s3] 
}


Script [bootstrap.sh]

O que está correto e validado:

Caminho do Python (/home/hadoop/conda): O script instala o Miniconda exatamente onde o seu configurations_json (no Terraform) espera encontrar. Isso evita o erro 
de "caminho não encontrado".

Bibliotecas em Todos os Nós: Ao rodar conda install ... scikit-learn pandas, você garante que todos os nós (Master e Core/Task) tenham essas bibliotecas. Isso é 
crucial se você for usar UDFs (User Defined Functions) do Pandas ou executar modelos do Scikit-Learn distribuídos.

O "Pulo do Gato" para Execução Distribuída
Agora chegamos ao ponto mais delicado do seu projeto, onde a maioria dos pipelines de PySpark falha no deploy.

Você criou a pasta pipeline no bootstrap: mkdir $HOME/pipeline

Isso cria a pasta vazia em todos os nós. Ótimo.

Porém, no seu Terraform (IaC/modules/emr/main.tf), o passo de cópia (aws s3 cp ...) é um Step. Steps no EMR geralmente rodam apenas no Nó Master.

